{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install datasets\n!pip install transformers","metadata":{"id":"0xQiIRuDWzUk","outputId":"cf357aca-d5d3-4071-df0a-2a65389d2fbc","execution":{"iopub.status.busy":"2022-09-07T10:27:35.316371Z","iopub.execute_input":"2022-09-07T10:27:35.317725Z","iopub.status.idle":"2022-09-07T10:27:56.896804Z","shell.execute_reply.started":"2022-09-07T10:27:35.317600Z","shell.execute_reply":"2022-09-07T10:27:56.895621Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.7/site-packages (2.1.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (4.64.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.5.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.13)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets) (4.12.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from datasets) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.21.6)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets) (3.0.0)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.28.1)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.8.1)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (5.0.0)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.3.5)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets) (3.8.1)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2022.5.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.1.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->datasets) (3.0.9)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (3.3)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.1.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2022.6.15)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.7.2)\nRequirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (0.13.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (21.4.0)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (4.0.2)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (6.0.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.3.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.2.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.8.0)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2022.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.20.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.8.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.1)\nRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.12.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.7.1)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.12.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.6.15)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.3)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.9)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset, load_metric\nfrom transformers import AutoConfig, Wav2Vec2Processor\nimport torchaudio\nimport torch","metadata":{"id":"NAfHBclEMnPh","execution":{"iopub.status.busy":"2022-09-07T10:28:28.997079Z","iopub.execute_input":"2022-09-07T10:28:28.997821Z","iopub.status.idle":"2022-09-07T10:28:31.761480Z","shell.execute_reply.started":"2022-09-07T10:28:28.997781Z","shell.execute_reply":"2022-09-07T10:28:31.760276Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import pathlib \nimport subprocess\nimport IPython\nimport pandas as pd\nimport librosa","metadata":{"id":"AYaGrK_SZrAy","execution":{"iopub.status.busy":"2022-09-07T10:30:00.363073Z","iopub.execute_input":"2022-09-07T10:30:00.364011Z","iopub.status.idle":"2022-09-07T10:30:12.035231Z","shell.execute_reply.started":"2022-09-07T10:30:00.363973Z","shell.execute_reply":"2022-09-07T10:30:12.034241Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"base_dir = pathlib.Path('../input/vgaf-dataset')","metadata":{"id":"aF6aNx6Argw8","execution":{"iopub.status.busy":"2022-09-07T10:30:13.350358Z","iopub.execute_input":"2022-09-07T10:30:13.351380Z","iopub.status.idle":"2022-09-07T10:30:13.356693Z","shell.execute_reply.started":"2022-09-07T10:30:13.351335Z","shell.execute_reply":"2022-09-07T10:30:13.355525Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train_path = base_dir/'Train_VGAF_audio-20220823T161233Z-001/Train_VGAF_audio'\nval_path = base_dir/'Val_VGAF_audio-20220823T161237Z-001/Val_VGAF_audio'","metadata":{"id":"Qdbt_oBJrlgI","execution":{"iopub.status.busy":"2022-09-07T10:30:13.792057Z","iopub.execute_input":"2022-09-07T10:30:13.792541Z","iopub.status.idle":"2022-09-07T10:30:13.798010Z","shell.execute_reply.started":"2022-09-07T10:30:13.792496Z","shell.execute_reply":"2022-09-07T10:30:13.796925Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"classes = ['Positive', 'Neutral', 'Negative']\nclass_to_idx = {'Positive': 1, 'Neutral': 2, 'Negative': 3}\nidx_to_class = {1: 'Positive', 2: 'Neutral', 3: 'Negative'}","metadata":{"id":"YA9WJXT5ekkZ","execution":{"iopub.status.busy":"2022-09-07T10:30:14.248290Z","iopub.execute_input":"2022-09-07T10:30:14.249105Z","iopub.status.idle":"2022-09-07T10:30:14.254880Z","shell.execute_reply.started":"2022-09-07T10:30:14.249068Z","shell.execute_reply":"2022-09-07T10:30:14.253374Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Audio extraction ","metadata":{"id":"x5qTKubdLOac"}},{"cell_type":"code","source":"audio_path = train_path.with_name('Train_VGAF_audio')\nprint(audio_path)\nfor video_path in train_path.iterdir():\n    #print(video_path)\n    subprocess.run(f'ffmpeg -i {video_path} -f wav -ab 192000 -ar 16000 -vn {audio_path/pathlib.Path(video_path.name).with_suffix(\".wav\")}', shell=True, check=True)\n    # print(audio_path/pathlib.Path(video_path.name).with_suffix('.wav'))","metadata":{"id":"QBWm8sQsjtQj","outputId":"457b6b07-eee4-40e8-93db-9b5df124777a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"audio_path = val_path.with_name('Val_VGAF_audio')\nprint(audio_path)\nfor video_path in val_path.iterdir():\n    print(video_path)\n    subprocess.run(f'ffmpeg -i {video_path} -f wav -ab 192000 -ar 16000 -vn {audio_path/pathlib.Path(video_path.name).with_suffix(\".wav\")}', shell=True, check=True)\n    # print(audio_path/pathlib.Path(video_path.name).with_suffix('.wav'))","metadata":{"id":"Zxu_jHop7OAw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dict_to_df_val = {'audio': [] ,'label': []} \nfor emotion_name in emotions_list: \n    test_videos_path = val_path/emotion_name\n    audio_path = val_path.with_name('Val_AFEW_audio')/emotion_name\n    curr_emotion = audio_path.name\n    for audio in audio_path.iterdir():\n        dict_to_df_val['audio'].append(curr_emotion + '/' + audio.name)\n        dict_to_df_val['label'].append(class_to_idx.get(curr_emotion))  ","metadata":{"id":"DL2UUNMOQZM4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Wav2Vec2","metadata":{"id":"tCFyix4HLqxX"}},{"cell_type":"code","source":"data_files = {\n    \"train\": \"../input/5-folds-vgaf/5_train_fold.txt\", \n    \"test\": \"../input/5-folds-vgaf/5_test_fold.txt\",\n    \"validation\": \"../input/vgaf-dataset/Val_labels.txt\",\n}\n\ndataset = load_dataset(\"csv\", data_files=data_files, delimiter=\" \", )\ntrain_dataset = dataset[\"train\"]\ntest_dataset = dataset[\"test\"]\neval_dataset = dataset[\"validation\"]\n\nprint(train_dataset)\nprint(test_dataset)\nprint(eval_dataset)","metadata":{"id":"RnmSQQQRBcqs","outputId":"b06f6c89-7362-44e4-8ce8-5f324880bc56","execution":{"iopub.status.busy":"2022-09-07T10:30:30.023916Z","iopub.execute_input":"2022-09-07T10:30:30.024321Z","iopub.status.idle":"2022-09-07T10:30:30.597826Z","shell.execute_reply.started":"2022-09-07T10:30:30.024280Z","shell.execute_reply":"2022-09-07T10:30:30.596849Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-097bf2f1ed0d3fc7/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfabf78b468d49a58e705be9d6be8908"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfec50392d7f45e4908beb52a73a3583"}},"metadata":{}},{"name":"stdout","text":"Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-097bf2f1ed0d3fc7/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f08901ffe23d4ead88c910393d51098b"}},"metadata":{}},{"name":"stdout","text":"Dataset({\n    features: ['Vid_name', 'Label'],\n    num_rows: 2129\n})\nDataset({\n    features: ['Vid_name', 'Label'],\n    num_rows: 532\n})\nDataset({\n    features: ['Vid_name', 'Label'],\n    num_rows: 766\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"# We need to specify the input and output column\ninput_column = \"Vid_name\"\noutput_column = \"Label\"","metadata":{"id":"DJj_ln9IDhVG","execution":{"iopub.status.busy":"2022-09-07T10:30:36.121767Z","iopub.execute_input":"2022-09-07T10:30:36.122889Z","iopub.status.idle":"2022-09-07T10:30:36.127681Z","shell.execute_reply.started":"2022-09-07T10:30:36.122841Z","shell.execute_reply":"2022-09-07T10:30:36.126454Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# we need to distinguish the unique labels in our SER dataset\nlabel_list = train_dataset.unique(output_column)\nlabel_list.sort()  # Let's sort it for determinism\nnum_labels = len(label_list)\nprint(f\"A classification problem with {num_labels} classes: {label_list}\")","metadata":{"id":"Qj0mP41wDhx4","outputId":"e740ed0e-9ac0-4ec4-ed67-e18ef0eae716","execution":{"iopub.status.busy":"2022-09-07T10:30:37.987829Z","iopub.execute_input":"2022-09-07T10:30:37.988190Z","iopub.status.idle":"2022-09-07T10:30:37.997403Z","shell.execute_reply.started":"2022-09-07T10:30:37.988162Z","shell.execute_reply":"2022-09-07T10:30:37.995322Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"A classification problem with 3 classes: [0, 1, 2]\n","output_type":"stream"}]},{"cell_type":"code","source":"model_name_or_path = \"jonatasgrosman/wav2vec2-large-xlsr-53-english\"\npooling_mode = \"mean\"","metadata":{"id":"--LdJUGYDh0T","execution":{"iopub.status.busy":"2022-09-07T10:30:41.392804Z","iopub.execute_input":"2022-09-07T10:30:41.393490Z","iopub.status.idle":"2022-09-07T10:30:41.397620Z","shell.execute_reply.started":"2022-09-07T10:30:41.393453Z","shell.execute_reply":"2022-09-07T10:30:41.396628Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# config\nconfig = AutoConfig.from_pretrained(\n    model_name_or_path,\n    num_labels=num_labels,\n    label2id=class_to_idx,\n    id2label=idx_to_class,\n    finetuning_task=\"wav2vec2_clf\",    \n)\nsetattr(config, 'pooling_mode', pooling_mode)","metadata":{"id":"6TeSgKHKDh2y","outputId":"da1e8313-4c3e-437b-b601-8d2c2cd1bbfe","execution":{"iopub.status.busy":"2022-09-07T10:30:44.760660Z","iopub.execute_input":"2022-09-07T10:30:44.761354Z","iopub.status.idle":"2022-09-07T10:30:45.515786Z","shell.execute_reply.started":"2022-09-07T10:30:44.761318Z","shell.execute_reply":"2022-09-07T10:30:45.514633Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.50k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1f9a66cecfd46bc9bc7987782843819"}},"metadata":{}}]},{"cell_type":"code","source":"processor = Wav2Vec2Processor.from_pretrained(model_name_or_path)\ntarget_sampling_rate = processor.feature_extractor.sampling_rate\nprint(f\"The target sampling rate: {target_sampling_rate}\")","metadata":{"id":"vD1bTgVZDh5t","outputId":"2980f8c2-6808-49d0-b144-bef14b6922f5","execution":{"iopub.status.busy":"2022-09-07T10:30:49.148331Z","iopub.execute_input":"2022-09-07T10:30:49.149375Z","iopub.status.idle":"2022-09-07T10:30:52.911090Z","shell.execute_reply.started":"2022-09-07T10:30:49.149327Z","shell.execute_reply":"2022-09-07T10:30:52.909915Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/262 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de3f396f95f843ae81a6e0ed70a81b4e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/300 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bd26ca6ca734124ad0094487c9c8055"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/85.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7bc111d30ee4d848645ba51c928e6d9"}},"metadata":{}},{"name":"stdout","text":"The target sampling rate: 16000\n","output_type":"stream"}]},{"cell_type":"code","source":"def speech_file_to_array_fn(path):\n    path += '.wav'\n    #print(train_path/path)\n    try:\n      speech_array, sampling_rate = librosa.load(train_path/path, sr = 16000)\n    except FileNotFoundError:\n      speech_array, sampling_rate = librosa.load(val_path/path, sr = 16000)\n    # resampler = torchaudio.transforms.Resample(sampling_rate, target_sampling_rate)\n    speech = speech_array\n    return speech\n\ndef label_to_id(label, label_list):\n\n    if len(label_list) > 0:\n        return label_list.index(label) if label in label_list else -1\n\n    return label\n\ndef preprocess_function(examples):   \n    # print(target_list)\n    speech_list = [speech_file_to_array_fn(path) for path in examples[input_column]]\n    #print(speech_list)\n    target_list = [label_to_id(label, label_list) for label in examples[output_column]]\n    # print(len(speech_list))\n    \n    # print(speech_list[0].shape)\n    result = processor(speech_list, sampling_rate=target_sampling_rate,  max_length=60000, padding=\"max_length\", truncation='longest_first', return_attention_mask=True)\n    result[\"labels\"] = list(target_list)\n    # print(result)\n    return result","metadata":{"id":"GraV0VlUFyC2","execution":{"iopub.status.busy":"2022-09-07T10:30:55.547575Z","iopub.execute_input":"2022-09-07T10:30:55.548255Z","iopub.status.idle":"2022-09-07T10:30:55.556332Z","shell.execute_reply.started":"2022-09-07T10:30:55.548220Z","shell.execute_reply":"2022-09-07T10:30:55.555345Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"train_dataset = train_dataset.map(\n    preprocess_function,\n    batch_size=30,\n    batched=True,\n    num_proc=4\n)\n","metadata":{"id":"J0z9zg4AXop-","outputId":"8b39f278-8d38-4cff-ff10-525b9c4462d3","execution":{"iopub.status.busy":"2022-09-07T10:31:00.920835Z","iopub.execute_input":"2022-09-07T10:31:00.921464Z","iopub.status.idle":"2022-09-07T10:35:44.352956Z","shell.execute_reply.started":"2022-09-07T10:31:00.921428Z","shell.execute_reply":"2022-09-07T10:35:44.351827Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"      ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#0:   0%|          | 0/18 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f5fd7375cd4419f9829da7c8eddd884"}},"metadata":{}},{"name":"stdout","text":" ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#1:   0%|          | 0/18 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"866b598217194a8697d522cfac877f25"}},"metadata":{}},{"name":"stdout","text":" ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#2:   0%|          | 0/18 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8e3b50d19ed4f28b4af7bd8c1faeded"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#3:   0%|          | 0/18 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e85d959626a44801816fdbb2803032f2"}},"metadata":{}}]},{"cell_type":"code","source":"# idx = 0\n# print(f\"Training input_values: {train_dataset[idx]['input_values']}\")\n# print(f\"Training attention_mask: {train_dataset[idx]['attention_mask']}\")\n# print(f\"Training labels: {train_dataset[idx]['labels']} - {train_dataset[idx]['Label']}\")","metadata":{"id":"cC8DLrK9FyKK","execution":{"iopub.status.busy":"2022-08-24T16:32:24.008658Z","iopub.execute_input":"2022-08-24T16:32:24.009302Z","iopub.status.idle":"2022-08-24T16:32:24.017749Z","shell.execute_reply.started":"2022-08-24T16:32:24.009258Z","shell.execute_reply":"2022-08-24T16:32:24.014630Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = test_dataset.map(\n    preprocess_function,\n    batch_size=30,\n    batched=True,\n    num_proc=4\n)\n","metadata":{"execution":{"iopub.status.busy":"2022-09-07T10:35:44.355239Z","iopub.execute_input":"2022-09-07T10:35:44.355630Z","iopub.status.idle":"2022-09-07T10:36:56.558431Z","shell.execute_reply.started":"2022-09-07T10:35:44.355593Z","shell.execute_reply":"2022-09-07T10:36:56.557323Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"      ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#0:   0%|          | 0/5 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d890d6eab74c4d52a0d3e8ab10585e48"}},"metadata":{}},{"name":"stdout","text":" ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#1:   0%|          | 0/5 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"273e87a5b766467fa8438809cb7e8ca3"}},"metadata":{}},{"name":"stdout","text":" ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#2:   0%|          | 0/5 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a50613026111442fa4a740f48509ce8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#3:   0%|          | 0/5 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d39e394695514caebd7cbaefb1a7e83e"}},"metadata":{}}]},{"cell_type":"code","source":"eval_dataset = eval_dataset.map(\n    preprocess_function,\n    batch_size=30,\n    batched=True,\n    num_proc=4\n)","metadata":{"id":"n5_0YZsiXaUF","outputId":"5ef216dd-c7a0-4353-f108-ef4631f356f2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForAudioClassification, TrainingArguments, Trainer\n\nmodel = AutoModelForAudioClassification.from_pretrained(\n    model_name_or_path, \n    num_labels=num_labels,\n    label2id=class_to_idx,\n    id2label=idx_to_class,\n)","metadata":{"id":"uUsJ-VGpXAPX","outputId":"31a483c3-4759-43c1-924c-8d7474cd50ef","execution":{"iopub.status.busy":"2022-09-07T10:36:56.562402Z","iopub.execute_input":"2022-09-07T10:36:56.563242Z","iopub.status.idle":"2022-09-07T10:37:55.774451Z","shell.execute_reply.started":"2022-09-07T10:36:56.563202Z","shell.execute_reply":"2022-09-07T10:37:55.773313Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.18G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13b91df2456f4b629132f2a219f94916"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at jonatasgrosman/wav2vec2-large-xlsr-53-english were not used when initializing Wav2Vec2ForSequenceClassification: ['lm_head.weight', 'lm_head.bias']\n- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at jonatasgrosman/wav2vec2-large-xlsr-53-english and are newly initialized: ['projector.bias', 'classifier.weight', 'projector.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"model_name = model_name_or_path.split(\"/\")[-1]\n\nargs = TrainingArguments(\n    f\"{model_name}-finetuned-ks\",\n    evaluation_strategy = \"epoch\",\n    save_strategy = \"epoch\",\n    learning_rate=1e-4,\n    per_device_train_batch_size=10,\n    gradient_accumulation_steps=2,\n    per_device_eval_batch_size=10,\n    num_train_epochs=8,\n    warmup_ratio=0.5,\n    logging_steps=10,\n    save_total_limit = 2,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    # push_to_hub=True,\n)","metadata":{"id":"ywNG__-HZJm1","execution":{"iopub.status.busy":"2022-09-07T10:37:55.780355Z","iopub.execute_input":"2022-09-07T10:37:55.782944Z","iopub.status.idle":"2022-09-07T10:37:55.886888Z","shell.execute_reply.started":"2022-09-07T10:37:55.782903Z","shell.execute_reply":"2022-09-07T10:37:55.885712Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"metric = load_metric(\"accuracy\")","metadata":{"id":"ULPDnFr8acGo","outputId":"1f4745de-766d-4978-8cc0-c480461af423","execution":{"iopub.status.busy":"2022-09-07T10:37:55.888330Z","iopub.execute_input":"2022-09-07T10:37:55.888733Z","iopub.status.idle":"2022-09-07T10:37:56.784783Z","shell.execute_reply.started":"2022-09-07T10:37:55.888692Z","shell.execute_reply":"2022-09-07T10:37:56.783919Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/1.41k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88ca4d2496b643e4a052461edcbd2500"}},"metadata":{}}]},{"cell_type":"code","source":"import numpy as np\n\ndef compute_metrics(eval_pred):\n    \"\"\"Computes accuracy on a batch of predictions\"\"\"\n    predictions = np.argmax(eval_pred.predictions, axis=1)\n    return metric.compute(predictions=predictions, references=eval_pred.label_ids)","metadata":{"id":"lJOkCE26ZJsu","execution":{"iopub.status.busy":"2022-09-07T10:37:56.786161Z","iopub.execute_input":"2022-09-07T10:37:56.786651Z","iopub.status.idle":"2022-09-07T10:37:56.793507Z","shell.execute_reply.started":"2022-09-07T10:37:56.786613Z","shell.execute_reply":"2022-09-07T10:37:56.792252Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    model,\n    args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    tokenizer=processor,\n    compute_metrics=compute_metrics\n)","metadata":{"id":"cbXLePrfZJvw","execution":{"iopub.status.busy":"2022-09-07T10:37:56.794826Z","iopub.execute_input":"2022-09-07T10:37:56.795873Z","iopub.status.idle":"2022-09-07T10:38:02.190604Z","shell.execute_reply.started":"2022-09-07T10:37:56.795837Z","shell.execute_reply":"2022-09-07T10:38:02.189455Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"id":"TAwjSPEza2ib","outputId":"f53a7070-803e-4a1a-86c1-86e862231279","execution":{"iopub.status.busy":"2022-09-07T10:38:02.192497Z","iopub.execute_input":"2022-09-07T10:38:02.192962Z","iopub.status.idle":"2022-09-07T11:26:17.244923Z","shell.execute_reply.started":"2022-09-07T10:38:02.192915Z","shell.execute_reply":"2022-09-07T11:26:17.243795Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"The following columns in the training set don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: Label, Vid_name. If Label, Vid_name are not expected by `Wav2Vec2ForSequenceClassification.forward`,  you can safely ignore this message.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 2129\n  Num Epochs = 8\n  Instantaneous batch size per device = 10\n  Total train batch size (w. parallel, distributed & accumulation) = 20\n  Gradient Accumulation steps = 2\n  Total optimization steps = 848\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.13.2 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.12.21"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20220907_104017-29h4qf92</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href=\"https://wandb.ai/amirabdrahimov/huggingface/runs/29h4qf92\" target=\"_blank\">wav2vec2-large-xlsr-53-english-finetuned-ks</a></strong> to <a href=\"https://wandb.ai/amirabdrahimov/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='848' max='848' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [848/848 45:46, Epoch 7/8]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>1.045500</td>\n      <td>1.027216</td>\n      <td>0.477444</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.936600</td>\n      <td>1.006037</td>\n      <td>0.526316</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.771300</td>\n      <td>1.009316</td>\n      <td>0.537594</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.776800</td>\n      <td>1.143799</td>\n      <td>0.522556</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.612300</td>\n      <td>1.423424</td>\n      <td>0.509398</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.488500</td>\n      <td>1.611376</td>\n      <td>0.528195</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.244800</td>\n      <td>1.826059</td>\n      <td>0.488722</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.149600</td>\n      <td>1.937706</td>\n      <td>0.481203</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: Label, Vid_name. If Label, Vid_name are not expected by `Wav2Vec2ForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 532\n  Batch size = 10\nSaving model checkpoint to wav2vec2-large-xlsr-53-english-finetuned-ks/checkpoint-106\nConfiguration saved in wav2vec2-large-xlsr-53-english-finetuned-ks/checkpoint-106/config.json\nModel weights saved in wav2vec2-large-xlsr-53-english-finetuned-ks/checkpoint-106/pytorch_model.bin\nFeature extractor saved in wav2vec2-large-xlsr-53-english-finetuned-ks/checkpoint-106/preprocessor_config.json\ntokenizer config file saved in wav2vec2-large-xlsr-53-english-finetuned-ks/checkpoint-106/tokenizer_config.json\nSpecial tokens file saved in wav2vec2-large-xlsr-53-english-finetuned-ks/checkpoint-106/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: Label, Vid_name. If Label, Vid_name are not expected by `Wav2Vec2ForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 532\n  Batch size = 10\nSaving model checkpoint to wav2vec2-large-xlsr-53-english-finetuned-ks/checkpoint-212\nConfiguration saved in wav2vec2-large-xlsr-53-english-finetuned-ks/checkpoint-212/config.json\nModel weights saved in wav2vec2-large-xlsr-53-english-finetuned-ks/checkpoint-212/pytorch_model.bin\nFeature extractor saved in wav2vec2-large-xlsr-53-english-finetuned-ks/checkpoint-212/preprocessor_config.json\ntokenizer config file saved in wav2vec2-large-xlsr-53-english-finetuned-ks/checkpoint-212/tokenizer_config.json\nSpecial tokens file saved in wav2vec2-large-xlsr-53-english-finetuned-ks/checkpoint-212/special_tokens_map.json\nThe following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: Label, Vid_name. If Label, Vid_name are not expected by `Wav2Vec2ForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 532\n  Batch size = 10\nSaving model checkpoint to wav2vec2-large-xlsr-53-english-finetuned-ks/checkpoint-318\nConfiguration saved in wav2vec2-large-xlsr-53-english-finetuned-ks/checkpoint-318/config.json\nModel weights saved in wav2vec2-large-xlsr-53-english-finetuned-ks/checkpoint-318/pytorch_model.bin\nFeature extractor saved in wav2vec2-large-xlsr-53-english-finetuned-ks/checkpoint-318/preprocessor_config.json\ntokenizer config file saved in wav2vec2-large-xlsr-53-english-finetuned-ks/checkpoint-318/tokenizer_config.json\nSpecial tokens file saved in wav2vec2-large-xlsr-53-english-finetuned-ks/checkpoint-318/special_tokens_map.json\nDeleting older checkpoint [wav2vec2-large-xlsr-53-english-finetuned-ks/checkpoint-106] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: Label, Vid_name. If Label, Vid_name are not expected by `Wav2Vec2ForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 532\n  Batch size = 10\nSaving model checkpoint to wav2vec2-large-xlsr-53-english-finetuned-ks/checkpoint-424\nConfiguration saved in wav2vec2-large-xlsr-53-english-finetuned-ks/checkpoint-424/config.json\nModel weights saved in wav2vec2-large-xlsr-53-english-finetuned-ks/checkpoint-424/pytorch_model.bin\nFeature extractor saved in wav2vec2-large-xlsr-53-english-finetuned-ks/checkpoint-424/preprocessor_config.json\ntokenizer config file saved in wav2vec2-large-xlsr-53-english-finetuned-ks/checkpoint-424/tokenizer_config.json\nSpecial tokens file saved in wav2vec2-large-xlsr-53-english-finetuned-ks/checkpoint-424/special_tokens_map.json\nDeleting older checkpoint [wav2vec2-large-xlsr-53-english-finetuned-ks/checkpoint-212] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: Label, Vid_name. If Label, Vid_name are not expected by `Wav2Vec2ForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 532\n  Batch size = 10\nSaving model checkpoint to wav2vec2-large-xlsr-53-english-finetuned-ks/checkpoint-530\nConfiguration saved in wav2vec2-large-xlsr-53-english-finetuned-ks/checkpoint-530/config.json\nModel weights saved in wav2vec2-large-xlsr-53-english-finetuned-ks/checkpoint-530/pytorch_model.bin\nFeature extractor saved in wav2vec2-large-xlsr-53-english-finetuned-ks/checkpoint-530/preprocessor_config.json\ntokenizer config file saved in wav2vec2-large-xlsr-53-english-finetuned-ks/checkpoint-530/tokenizer_config.json\nSpecial tokens file saved in wav2vec2-large-xlsr-53-english-finetuned-ks/checkpoint-530/special_tokens_map.json\nDeleting older checkpoint [wav2vec2-large-xlsr-53-english-finetuned-ks/checkpoint-424] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: Label, Vid_name. If Label, Vid_name are not expected by `Wav2Vec2ForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 532\n  Batch size = 10\nSaving model checkpoint to wav2vec2-large-xlsr-53-english-finetuned-ks/checkpoint-636\nConfiguration saved in wav2vec2-large-xlsr-53-english-finetuned-ks/checkpoint-636/config.json\nModel weights saved in wav2vec2-large-xlsr-53-english-finetuned-ks/checkpoint-636/pytorch_model.bin\nFeature extractor saved in wav2vec2-large-xlsr-53-english-finetuned-ks/checkpoint-636/preprocessor_config.json\ntokenizer config file saved in wav2vec2-large-xlsr-53-english-finetuned-ks/checkpoint-636/tokenizer_config.json\nSpecial tokens file saved in wav2vec2-large-xlsr-53-english-finetuned-ks/checkpoint-636/special_tokens_map.json\nDeleting older checkpoint [wav2vec2-large-xlsr-53-english-finetuned-ks/checkpoint-530] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: Label, Vid_name. If Label, Vid_name are not expected by `Wav2Vec2ForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 532\n  Batch size = 10\nSaving model checkpoint to wav2vec2-large-xlsr-53-english-finetuned-ks/checkpoint-742\nConfiguration saved in wav2vec2-large-xlsr-53-english-finetuned-ks/checkpoint-742/config.json\nModel weights saved in wav2vec2-large-xlsr-53-english-finetuned-ks/checkpoint-742/pytorch_model.bin\nFeature extractor saved in wav2vec2-large-xlsr-53-english-finetuned-ks/checkpoint-742/preprocessor_config.json\ntokenizer config file saved in wav2vec2-large-xlsr-53-english-finetuned-ks/checkpoint-742/tokenizer_config.json\nSpecial tokens file saved in wav2vec2-large-xlsr-53-english-finetuned-ks/checkpoint-742/special_tokens_map.json\nDeleting older checkpoint [wav2vec2-large-xlsr-53-english-finetuned-ks/checkpoint-636] due to args.save_total_limit\nThe following columns in the evaluation set don't have a corresponding argument in `Wav2Vec2ForSequenceClassification.forward` and have been ignored: Label, Vid_name. If Label, Vid_name are not expected by `Wav2Vec2ForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running Evaluation *****\n  Num examples = 532\n  Batch size = 10\nSaving model checkpoint to wav2vec2-large-xlsr-53-english-finetuned-ks/checkpoint-848\nConfiguration saved in wav2vec2-large-xlsr-53-english-finetuned-ks/checkpoint-848/config.json\nModel weights saved in wav2vec2-large-xlsr-53-english-finetuned-ks/checkpoint-848/pytorch_model.bin\nFeature extractor saved in wav2vec2-large-xlsr-53-english-finetuned-ks/checkpoint-848/preprocessor_config.json\ntokenizer config file saved in wav2vec2-large-xlsr-53-english-finetuned-ks/checkpoint-848/tokenizer_config.json\nSpecial tokens file saved in wav2vec2-large-xlsr-53-english-finetuned-ks/checkpoint-848/special_tokens_map.json\nDeleting older checkpoint [wav2vec2-large-xlsr-53-english-finetuned-ks/checkpoint-742] due to args.save_total_limit\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from wav2vec2-large-xlsr-53-english-finetuned-ks/checkpoint-318 (score: 0.5375939849624061).\n","output_type":"stream"},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=848, training_loss=0.6026811416981355, metrics={'train_runtime': 2894.9706, 'train_samples_per_second': 5.883, 'train_steps_per_second': 0.293, 'total_flos': 1.9347095845774802e+18, 'train_loss': 0.6026811416981355, 'epoch': 8.0})"},"metadata":{}}]},{"cell_type":"code","source":"import shutil\nshutil.make_archive('w2v2_l_5f_0.53', 'zip', 'wav2vec2-large-xlsr-53-english-finetuned-ks/checkpoint-318')","metadata":{"id":"PfniFrcLa2p2","execution":{"iopub.status.busy":"2022-09-07T11:28:47.719325Z","iopub.execute_input":"2022-09-07T11:28:47.719740Z","iopub.status.idle":"2022-09-07T11:32:38.547972Z","shell.execute_reply.started":"2022-09-07T11:28:47.719709Z","shell.execute_reply":"2022-09-07T11:32:38.547057Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/w2v2_l_5f_0.53.zip'"},"metadata":{}}]},{"cell_type":"code","source":"\n","metadata":{},"execution_count":null,"outputs":[]}]}
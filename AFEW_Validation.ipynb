{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4c8cbe5",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41ee9436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from openvino.runtime import Core\n",
    "import os\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6800556c",
   "metadata": {},
   "source": [
    "## Evaluate image with OpenVino model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e3c4d6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ie = Core()\n",
    "model = ie.read_model(model=os.path.normpath(r\"D:\\Users\\amira\\openvino_notebooks\\notebooks\\accuracy_afew\\models\\omz_emotion_recognition_model\\emotion-recognition-retail-0003.xml\"))\n",
    "compiled_model = ie.compile_model(model=model, device_name=\"CPU\")\n",
    "\n",
    "output_layer = next(iter(compiled_model.outputs))\n",
    "input_layer = next(iter(compiled_model.inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "330cdc73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ConstOutput: names[data] shape{1,3,64,64} type: f32>\n",
      "<ConstOutput: names[prob_emotion] shape{1,5,1,1} type: f32>\n"
     ]
    }
   ],
   "source": [
    "print(input_layer)\n",
    "print(output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d8fc67d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ovm_classes_to_idx = {'Neutral':0, 'Happy':1, 'Sad':2, 'Surprise':3, 'Angry':4}\n",
    "idx_to_ovm_classes = {0:'Neutral', 1:'Happy', 2:'Sad', 3:'Surprise', 4:'Angry'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "eca45b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The MobileNet model expects images in RGB format\n",
    "image = cv2.cvtColor(cv2.imread(filename=\"data/scared.jpg\"), code=cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# resize to MobileNet image shape\n",
    "input_image = cv2.resize(src=image, dsize=(64, 64))\n",
    "\n",
    "# reshape to model input shape\n",
    "input_image = np.expand_dims(input_image.transpose(2, 0, 1), 0)\n",
    "plt.imshow(image);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "afabeaff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Angry\n"
     ]
    }
   ],
   "source": [
    "result_infer = compiled_model(inputs=[input_image])[output_layer]\n",
    "result_index = np.argmax(result_infer)\n",
    "print(idx_to_ovm_classes.get(result_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc597350",
   "metadata": {},
   "source": [
    "## Evaluation of EfficientNet B0 in OpenVino - 13.69% Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "224ebf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "ie = Core()\n",
    "model = ie.read_model(model=os.path.normpath(r\"D:\\Users\\amira\\openvino_notebooks\\notebooks\\accuracy_afew\\models\\enet_b0_8\\FP32\\enet_b0_8.xml\"))\n",
    "compiled_model = ie.compile_model(model=model, device_name=\"CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c247ab9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ConstOutput: names[x.1] shape{1,3,224,224} type: f32>\n",
      "<ConstOutput: names[653] shape{1,8} type: f32>\n"
     ]
    }
   ],
   "source": [
    "output_layer = next(iter(compiled_model.outputs))\n",
    "input_layer = next(iter(compiled_model.inputs))\n",
    "print(input_layer)\n",
    "print(output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0b581d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_idx = {'Angry': 0, 'Contempt': 1, 'Disgust': 2, 'Fear': 3, 'Happy': 4, 'Neutral': 5, 'Sad': 6, 'Surprise': 7}\n",
    "idx_to_class = {0: 'Angry', 1: 'Contempt', 2: 'Disgust', 3: 'Fear', 4: 'Happy', 5: 'Neutral', 6: 'Sad', 7: 'Surprise'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2ee618dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR=r'D:\\Users\\amira\\Documents\\datasets\\emotions\\AudioVideo\\faces\\Val_AFEW'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d35cf56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforms(filepath):\n",
    "    image = cv2.cvtColor(cv2.imread(filename=filepath), code=cv2.COLOR_BGR2RGB)\n",
    "    input_image = cv2.resize(src=image, dsize=(224, 224))\n",
    "    input_image = np.expand_dims(input_image.transpose(2, 0, 1), 0)\n",
    "    return input_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4b631334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(input):\n",
    "    modified_input = np.subtract(input, input.max(axis=0, keepdims=True))\n",
    "\n",
    "    # Get unnormalized probabilities\n",
    "    exp_values = np.exp(modified_input - np.max(modified_input, axis=0, keepdims=True))\n",
    "    # Normalize them for each sample\n",
    "    probabilities = exp_values / np.sum(exp_values, axis=0, keepdims=True)\n",
    "\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "40374bd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7d49799073740418b5a8533f8fe4b51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8806b6217e824ea7bb4f22892f421cba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9168f65bb2114723a09a31e87e042bcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4ef0c7786aa40ec89d82332cf6cd475",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "720cf22c9fe34b55b896a31a0c34c41a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bddc02c848194deeac85cb3d1201e238",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c3d84a7d96b46d69ff5f6113c5107d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19962, 8) (19962,)\n"
     ]
    }
   ],
   "source": [
    "y_val,y_scores_val=[],[]\n",
    "\n",
    "for class_name in os.listdir(DATA_DIR):\n",
    "    if class_name in class_to_idx:\n",
    "        #print(class_name)\n",
    "        class_dir=os.path.join(DATA_DIR,class_name)\n",
    "        y=class_to_idx[class_name]\n",
    "        for video_name in tqdm(os.listdir(class_dir)):\n",
    "            video_dir = os.path.join(class_dir,video_name)\n",
    "            for img_name in os.listdir(video_dir):\n",
    "                if 'noface' not in img_name:\n",
    "                    filepath=os.path.join(video_dir,img_name)\n",
    "                    input_image = transforms(filepath)\n",
    "                    scores = compiled_model(inputs=[input_image])[output_layer]\n",
    "                    #print(scores.shape)\n",
    "                    y_scores_val.append(softmax(scores[0]))\n",
    "                    y_val.append(y)\n",
    "\n",
    "y_scores_val=np.array(y_scores_val)\n",
    "y_val=np.array(y_val)\n",
    "print(y_scores_val.shape,y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e618c701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.686003406472297\n",
      "Angry 563/3297 acc: 17.076130\n",
      "Contempt 0/0 acc: nan\n",
      "Disgust 44/2485 acc: 1.770624\n",
      "Fear 246/1703 acc: 14.445097\n",
      "Happy 1205/3320 acc: 36.295181\n",
      "Neutral 27/3693 acc: 0.731113\n",
      "Sad 618/3356 acc: 18.414779\n",
      "Surprise 29/2108 acc: 1.375712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\amira\\AppData\\Local\\Temp\\ipykernel_1104\\1344098004.py:8: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  _val_acc=(y_pred[y_val==i]==i).sum()/(y_val==i).sum()\n"
     ]
    }
   ],
   "source": [
    "y_pred=np.argmax(y_scores_val,axis=1)\n",
    "acc=100.0*(y_val==y_pred).sum()/len(y_val)\n",
    "print(acc)\n",
    "\n",
    "y_train=np.array(y_val)\n",
    "\n",
    "for i in range(y_scores_val.shape[1]):\n",
    "    _val_acc=(y_pred[y_val==i]==i).sum()/(y_val==i).sum()\n",
    "    print('%s %d/%d acc: %f' %(idx_to_class[i], (y_pred[y_val==i]==i).sum(), (y_val==i).sum(), 100*_val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3d4a72e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19962, 7)\n",
      "13.696022442641018\n"
     ]
    }
   ],
   "source": [
    "# Model was trained on 8 emotions on AffectNet so we have to delete 1 emotion that is not on AFEW\n",
    "сontempt_idx=class_to_idx['Contempt']\n",
    "y_scores_val_filtered=y_scores_val[:, [i!=сontempt_idx for i in idx_to_class]]\n",
    "print(y_scores_val_filtered.shape)\n",
    "y_pred=np.argmax(y_scores_val_filtered,axis=1)\n",
    "other_indices=y_val!=сontempt_idx\n",
    "y_val_new=np.array([y if y<сontempt_idx else y-1 for y in y_val if y!=сontempt_idx])\n",
    "acc=100.0*np.mean(y_val_new==y_pred[other_indices])\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccca4cdd",
   "metadata": {},
   "source": [
    "## Evaluation of OpenModelZoo Model [emotions-recognition-retail-0003](https://github.com/openvinotoolkit/open_model_zoo/tree/master/models/intel/emotions-recognition-retail-0003) (5 emotions) - 40.39%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24d92700",
   "metadata": {},
   "outputs": [],
   "source": [
    "ie = Core()\n",
    "model = ie.read_model(model=os.path.normpath(r\"D:\\Users\\amira\\openvino_notebooks\\notebooks\\accuracy_afew\\models\\omz_emotion_recognition_model\\emotion-recognition-retail-0003.xml\"))\n",
    "compiled_model = ie.compile_model(model=model, device_name=\"CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80d906e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ConstOutput: names[data] shape{1,3,64,64} type: f32>\n",
      "<ConstOutput: names[prob_emotion] shape{1,5,1,1} type: f32>\n"
     ]
    }
   ],
   "source": [
    "output_layer = next(iter(compiled_model.outputs))\n",
    "input_layer = next(iter(compiled_model.inputs))\n",
    "print(input_layer)\n",
    "print(output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f43907f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforms(filepath):\n",
    "    image = cv2.cvtColor(cv2.imread(filename=filepath), code=cv2.COLOR_BGR2RGB)\n",
    "    input_image = cv2.resize(src=image, dsize=(64, 64))\n",
    "    input_image = np.expand_dims(input_image.transpose(2, 0, 1), 0)\n",
    "    return input_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dde1fb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR=r'D:\\Users\\amira\\Documents\\datasets\\emotions\\AudioVideo\\faces\\Val_AFEW'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8d64712",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(input):\n",
    "    modified_input = np.subtract(input, input.max(axis=0, keepdims=True))\n",
    "\n",
    "    # Get unnormalized probabilities\n",
    "    exp_values = np.exp(modified_input - np.max(modified_input, axis=0, keepdims=True))\n",
    "    # Normalize them for each sample\n",
    "    probabilities = exp_values / np.sum(exp_values, axis=0, keepdims=True)\n",
    "\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9b52000",
   "metadata": {},
   "outputs": [],
   "source": [
    "ovm_classes_to_idx = {'Neutral':0, 'Happy':1, 'Sad':2, 'Surprise':3, 'Angry':4}\n",
    "idx_to_ovm_classes = {0:'Neutral', 1:'Happy', 2:'Sad', 3:'Surprise', 4:'Angry'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6381176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf609b460d5c4c138baef7dfea4cbe64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb4bbaa105de4b9b8f031143483ccf46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e697b7cc6f9499ea6a1ed7b12a41f04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6e24a6683b34a20b08d5b33fda133bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5be95d59cb1842ba8edf941e31887aaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15774, 5, 1, 1) (15774,)\n"
     ]
    }
   ],
   "source": [
    "y_val,y_scores_val=[],[]\n",
    "\n",
    "for class_name in os.listdir(DATA_DIR):\n",
    "    if class_name in ovm_classes_to_idx:\n",
    "        #print(class_name)\n",
    "        class_dir=os.path.join(DATA_DIR,class_name)\n",
    "        y=ovm_classes_to_idx[class_name]\n",
    "        for video_name in tqdm(os.listdir(class_dir)):\n",
    "            video_dir = os.path.join(class_dir,video_name)\n",
    "            for img_name in os.listdir(video_dir):\n",
    "                if 'noface' not in img_name:\n",
    "                    filepath=os.path.join(video_dir,img_name)\n",
    "                    input_image = transforms(filepath)\n",
    "                    scores = compiled_model(inputs=[input_image])[output_layer]\n",
    "                    #print(scores.shape)\n",
    "                    y_scores_val.append(softmax(scores[0]))\n",
    "                    y_val.append(y)\n",
    "\n",
    "y_scores_val=np.array(y_scores_val)\n",
    "y_val=np.array(y_val)\n",
    "print(y_scores_val.shape,y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "196d2778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.39558767592241\n",
      "Neutral 899/3693 acc: 24.343352\n",
      "Happy 1384/3320 acc: 41.686747\n",
      "Sad 2390/3356 acc: 71.215733\n",
      "Surprise 599/2108 acc: 28.415560\n",
      "Angry 1100/3297 acc: 33.363664\n"
     ]
    }
   ],
   "source": [
    "y_pred=np.argmax(y_scores_val,axis=1).squeeze()\n",
    "acc=100.0*(y_val==y_pred).sum()/len(y_val)\n",
    "print(acc)\n",
    "\n",
    "y_train=np.array(y_val)\n",
    "\n",
    "for i in range(y_scores_val.shape[1]):\n",
    "    _val_acc=(y_pred[y_val==i]==i).sum()/(y_val==i).sum()\n",
    "    print('%s %d/%d acc: %f' %(idx_to_ovm_classes[i], (y_pred[y_val==i]==i).sum(), (y_val==i).sum(), 100*_val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b364f3dd",
   "metadata": {},
   "source": [
    "## EfficientNet B0 before convertation to OpenVino (7 emotions - 36.59% / 5 emotions - 49.12%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b0664244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "05e26c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR=r'D:\\Users\\amira\\Documents\\datasets\\emotions\\AudioVideo\\faces\\Val_AFEW'\n",
    "IMG_SIZE = 224\n",
    "\n",
    "test_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((IMG_SIZE,IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0baff978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (conv_stem): Conv2dSame(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "  (bn1): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act1): SiLU(inplace=True)\n",
       "  (blocks): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): DepthwiseSeparableConv(\n",
       "        (conv_dw): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn1): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pw): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (conv_pw): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2dSame(96, 96, kernel_size=(3, 3), stride=(2, 2), groups=96, bias=False)\n",
       "        (bn2): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "        (bn2): BatchNorm2d(144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2dSame(144, 144, kernel_size=(5, 5), stride=(2, 2), groups=144, bias=False)\n",
       "        (bn2): BatchNorm2d(144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(40, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "        (bn2): BatchNorm2d(240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(40, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2dSame(240, 240, kernel_size=(3, 3), stride=(2, 2), groups=240, bias=False)\n",
       "        (bn2): BatchNorm2d(240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "        (bn2): BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "        (bn2): BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
       "        (bn2): BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "        (bn2): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "        (bn2): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2dSame(672, 672, kernel_size=(5, 5), stride=(2, 2), groups=672, bias=False)\n",
       "        (bn2): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "        (bn2): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "        (bn2): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): InvertedResidual(\n",
       "        (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "        (bn2): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "        (bn2): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv_head): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (bn2): BatchNorm2d(1280, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act2): SiLU(inplace=True)\n",
       "  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=True)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=1280, out_features=8, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_to_idx = {'Angry': 0, 'Contempt': 1, 'Disgust': 2, 'Fear': 3, 'Happy': 4, 'Neutral': 5, 'Sad': 6, 'Surprise': 7}\n",
    "idx_to_class = {0: 'Angry', 1: 'Contempt', 2: 'Disgust', 3: 'Fear', 4: 'Happy', 5: 'Neutral', 6: 'Sad', 7: 'Surprise'}\n",
    "emotion_to_index = {'Angry':0, 'Disgust':1, 'Fear':2, 'Happy':3, 'Neutral':4, 'Sad':5, 'Surprise':6}\n",
    "\n",
    "PATH = r'D:\\Users\\amira\\openvino_notebooks\\notebooks\\accuracy_afew\\models\\enet_b0_8\\enet_b0_8'\n",
    "device = 'cuda:0'\n",
    "model = torch.load(PATH)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "60e1c5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(input):\n",
    "    modified_input = np.subtract(input, input.max(axis=0, keepdims=True))\n",
    "\n",
    "    # Get unnormalized probabilities\n",
    "    exp_values = np.exp(modified_input - np.max(modified_input, axis=0, keepdims=True))\n",
    "    # Normalize them for each sample\n",
    "    probabilities = exp_values / np.sum(exp_values, axis=0, keepdims=True)\n",
    "\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5e0cba21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d03fe56e764140078cb72a29a8c1a319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f036631f5b142bb8a9abf65b261ffb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41c8567ded5d44a5b6938f06ae71a136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0f224e24a744a34a7b3ca433f79c2dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7af836bf574b49f3b4c6e71603b564e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a1466d5a1f74658a510d91ad5cd3c8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed1bf3027e9b40458df9ea32d9421a71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19962, 8) (19962,)\n"
     ]
    }
   ],
   "source": [
    "y_val,y_scores_val=[],[]\n",
    "\n",
    "for class_name in os.listdir(DATA_DIR):\n",
    "    if class_name in class_to_idx:\n",
    "        class_dir=os.path.join(DATA_DIR,class_name)\n",
    "        y=class_to_idx[class_name]\n",
    "        for video_name in tqdm(os.listdir(class_dir)):\n",
    "            video_dir = os.path.join(class_dir,video_name)\n",
    "            for img_name in os.listdir(video_dir):\n",
    "                if 'noface' not in img_name:\n",
    "                    filepath=os.path.join(video_dir,img_name)\n",
    "                    img = Image.open(filepath)\n",
    "                    img_tensor = test_transforms(img)\n",
    "                    img_tensor.unsqueeze_(0)\n",
    "                    scores = model(img_tensor.to(device))\n",
    "                    scores=scores[0].data.cpu().numpy()\n",
    "                    #print(scores.shape)\n",
    "                    y_scores_val.append(softmax(scores))\n",
    "                    y_val.append(y)\n",
    "\n",
    "y_scores_val=np.array(y_scores_val)\n",
    "y_val=np.array(y_val)\n",
    "print(y_scores_val.shape,y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "94615c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.22192165113716\n",
      "Angry 1220/3297 acc: 37.003336\n",
      "Contempt 0/0 acc: nan\n",
      "Disgust 372/2485 acc: 14.969819\n",
      "Fear 671/1703 acc: 39.401057\n",
      "Happy 1577/3320 acc: 47.500000\n",
      "Neutral 798/3693 acc: 21.608448\n",
      "Sad 1910/3356 acc: 56.912992\n",
      "Surprise 483/2108 acc: 22.912713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\amira\\AppData\\Local\\Temp\\ipykernel_1104\\1864895788.py:8: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  _val_acc=(y_pred[y_val==i]==i).sum()/(y_val==i).sum()\n"
     ]
    }
   ],
   "source": [
    "y_pred=np.argmax(y_scores_val,axis=1)\n",
    "acc=100.0*(y_val==y_pred).sum()/len(y_val)\n",
    "print(acc)\n",
    "\n",
    "y_train=np.array(y_val)\n",
    "\n",
    "for i in range(y_scores_val.shape[1]):\n",
    "    _val_acc=(y_pred[y_val==i]==i).sum()/(y_val==i).sum()\n",
    "    print('%s %d/%d acc: %f' %(idx_to_class[i],(y_pred[y_val==i]==i).sum(),(y_val==i).sum(),100*_val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f4f3587e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19962, 7)\n",
      "36.59452960625188\n"
     ]
    }
   ],
   "source": [
    "# Model was trained on 8 emotions on AffectNet so we have to delete 1 emotion that is not on AFEW\n",
    "сontempt_idx=class_to_idx['Contempt']\n",
    "y_scores_val_filtered=y_scores_val[:, [i!=сontempt_idx for i in idx_to_class]]\n",
    "print(y_scores_val_filtered.shape)\n",
    "y_pred=np.argmax(y_scores_val_filtered,axis=1)\n",
    "other_indices=y_val!=сontempt_idx\n",
    "y_val_new=np.array([y if y<сontempt_idx else y-1 for y in y_val if y!=сontempt_idx])\n",
    "acc=100.0*np.mean(y_val_new==y_pred[other_indices])\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5b58fb",
   "metadata": {},
   "source": [
    "5 emotions that model from OpenVino's OMZ can classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b617425a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19962, 5)\n",
      "49.125142639786986\n"
     ]
    }
   ],
   "source": [
    "irrelevant_idxs = [class_to_idx['Contempt'], class_to_idx['Disgust'], class_to_idx['Fear']]\n",
    "\n",
    "y_scores_val_filtered=y_scores_val[:, [i not in irrelevant_idxs for i in idx_to_class]]\n",
    "print(y_scores_val_filtered.shape)\n",
    "\n",
    "y_pred=np.argmax(y_scores_val_filtered,axis=1)\n",
    "other_indices=[i not in irrelevant_idxs for i in y_val]\n",
    "y_val_new=np.array([y if y<class_to_idx['Contempt'] else y-3 for y in y_val if y not in irrelevant_idxs])\n",
    "acc=100.0*np.mean(y_val_new==y_pred[other_indices])\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53c6116",
   "metadata": {},
   "source": [
    "## Efficient Net b0 as feature extractor with classifier in OpenVino - 20.1%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cef7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from random import shuffle\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier,ExtraTreesClassifier\n",
    "from sklearn import svm,metrics,preprocessing\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import preprocessing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886cf912",
   "metadata": {},
   "source": [
    "### Convert to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a160ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = r'D:\\Users\\amira\\openvino_notebooks\\notebooks\\accuracy_afew\\models\\enet_b0_8\\enet_b0_8.pt'\n",
    "\n",
    "feature_extractor_model = torch.load(PATH)\n",
    "feature_extractor_model.classifier=torch.nn.Identity()\n",
    "feature_extractor_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7a3402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy input for the model. It will be used to run the model inside export function.\n",
    "dummy_input = torch.randn(1, 3, 224, 224).cuda()\n",
    "# Call the export function\n",
    "torch.onnx.export(feature_extractor_model, (dummy_input, ), 'enet_b0_no_cl.onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9e4e9e",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "211e31f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ConstOutput: names[x.1] shape{1,3,224,224} type: f32>\n",
      "<ConstOutput: names[650] shape{1,1280} type: f32>\n"
     ]
    }
   ],
   "source": [
    "ie = Core()\n",
    "model = ie.read_model(model=os.path.normpath(r\"D:\\Users\\amira\\openvino_notebooks\\notebooks\\accuracy_afew\\models\\enet_b0_8\\enet_b0_no_cl\\FP32\\enet_b0_no_cl.xml\"))\n",
    "feature_extractor = ie.compile_model(model=model, device_name=\"CPU\")\n",
    "output_layer = next(iter(feature_extractor.outputs))\n",
    "input_layer = next(iter(feature_extractor.inputs))\n",
    "print(input_layer)\n",
    "print(output_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92c9770b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR=r'D:\\Users\\amira\\Documents\\datasets\\emotions\\AudioVideo\\faces'\n",
    "IMG_SIZE = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "371ebda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforms(filepath):\n",
    "    image = cv2.cvtColor(cv2.imread(filename=filepath), code=cv2.COLOR_BGR2RGB)\n",
    "    input_image = cv2.resize(src=image, dsize=(224, 224))\n",
    "    input_image = np.expand_dims(input_image.transpose(2, 0, 1), 0)\n",
    "    return input_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5bbf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((IMG_SIZE,IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3dd6a90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_to_index = {'Angry':0, 'Disgust':1, 'Fear':2, 'Happy':3, 'Neutral':4, 'Sad':5, 'Surprise':6}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f86f4c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Angry\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55084367da0f48ad93a9adf65873d77a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/133 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disgust\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f65ac92c3ca4517adddfb8cf8cc21c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fear\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "447ac470a50f47eba5696e72dbc7d29a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Happy\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1efbb46905548e591324bf67f9abc4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neutral\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86c9b4158eb24785b316e782b14de9bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/144 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sad\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eb96b15342d4b30a1ecf21f59528399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/117 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surprise\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84e673e57db245a4a9c115e90eb3ebaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_features(data_dir):\n",
    "    filename2features={}\n",
    "    #video_scores=[]\n",
    "    for class_name in os.listdir(data_dir):\n",
    "        if class_name in emotion_to_index:\n",
    "            class_dir = os.path.join(data_dir, class_name)\n",
    "            print(class_name)\n",
    "        for video_name in tqdm(os.listdir(class_dir)):\n",
    "            video_scores=[]\n",
    "            frames_dir=os.path.join(class_dir,video_name)\n",
    "            X_isface=[]\n",
    "            for img_name in os.listdir(frames_dir):\n",
    "                filepath=os.path.join(frames_dir,img_name)\n",
    "                input_image = transforms(filepath)\n",
    "                X_isface.append('noface' not in img_name)\n",
    "                    \n",
    "                if input_image.size:\n",
    "                    scores = feature_extractor(inputs=[input_image])[output_layer]\n",
    "                    video_scores.append(scores)\n",
    "            filename2features[video_name]=(video_scores,X_isface)\n",
    "    return filename2features\n",
    "\n",
    "filename2features_val=get_features(os.path.join(DATA_DIR, 'Val_AFEW'))\n",
    "filename2features_train=get_features(os.path.join(DATA_DIR, 'Train_AFEW'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "31a256a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enet_b0_no_cl_feat_emotiw.pickle\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model_name = 'enet_b0_no_cl'\n",
    "MODEL2EMOTIW_FEATURES=model_name+'_feat_emotiw.pickle' \n",
    "\n",
    "print(MODEL2EMOTIW_FEATURES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a93a5bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(MODEL2EMOTIW_FEATURES, 'wb') as handle:\n",
    "    pickle.dump([filename2features_train,filename2features_val], handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1847fb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(MODEL2EMOTIW_FEATURES, 'rb') as handle:\n",
    "    filename2features_train,filename2features_val=pickle.load(handle)\n",
    "print(len(filename2features_train),len(filename2features_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "57765af1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b6a461d7cf44471b784ecd35e3a26cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/133 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eebf2fa8109471facf232cb0df87194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40e05d525e704875a963204fbc021765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c902cd5c0a84e3bb81328d51215aadd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2549aefb00c843d2b6e82e44a4caf780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/144 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4f32fe6174a40e6a27d5ff693f983f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/117 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dc2f83c73ec47299c469d52eea1dc73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(773, 5120) (773,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f30b68bf2ef4acaa6f8862314afc154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3ba6b5e96554bb1973d7065538fefa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb4d11dd73fb408aa69d72724b712992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4024c39ebf584ec397ef5664f8e0a8a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86b69dea1cb543a9846b81b9bb4a1132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dd1a5d2f553498dab5df67f2212c0b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4781cafbe6b402ca62e8c0fded23bee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(383, 5120) (383,)\n"
     ]
    }
   ],
   "source": [
    "def create_dataset(filename2features,data_dir):\n",
    "    x = []\n",
    "    y = []\n",
    "    has_faces=[]\n",
    "    ind=0\n",
    "    for class_name in emotion_to_index:\n",
    "        for filename in tqdm(os.listdir(os.path.join(data_dir,class_name))):\n",
    "            #print(filename)\n",
    "            fn=os.path.splitext(filename)[0] # goes through files names\n",
    "            if not fn in filename2features:\n",
    "                continue\n",
    "            features=filename2features[fn]\n",
    "            total_features=None\n",
    "            #print(len(features))\n",
    "            if True:\n",
    "                if len(features[0])!=0:\n",
    "                    #print(features)\n",
    "                    cur_features=features[0][features[-1]==1]\n",
    "                #print(prev,features.shape)\n",
    "            else:\n",
    "                cur_features=features[0]\n",
    "            if len(cur_features)==0:\n",
    "                has_faces.append(0)\n",
    "                total_features=np.zeros_like(feature)\n",
    "            else:\n",
    "                has_faces.append(1)\n",
    "                #mean_features=features.mean(axis=0)\n",
    "                mean_features = (np.mean(cur_features, axis=0))\n",
    "                std_features = (np.std(cur_features, axis=0))\n",
    "                max_features = (np.max(cur_features, axis=0))\n",
    "                min_features = (np.min(cur_features, axis=0))\n",
    "\n",
    "                # join several features together\n",
    "                feature = np.concatenate((mean_features, std_features, min_features, max_features), axis=None)                    \n",
    "                #feature = np.concatenate((mean_features, std_features, min_features), axis=None)\n",
    "                #feature = np.concatenate((mean_features, min_features, max_features), axis=None)\n",
    "                #feature = np.concatenate((max_features, std_features), axis=None)\n",
    "                #feature=max_features\n",
    "\n",
    "                total_features=feature\n",
    "            \n",
    "            if total_features is not None:\n",
    "                x.append(total_features)\n",
    "                y.append(emotion_to_index[class_name])\n",
    "    x=np.array(x)\n",
    "    y=np.array(y)\n",
    "    has_faces=np.array(has_faces)\n",
    "    print(x.shape,y.shape)\n",
    "    return x,y,has_faces\n",
    "\n",
    "x_train, y_train, has_faces_train = create_dataset(filename2features_train, os.path.join(DATA_DIR, 'Train_AFEW'))\n",
    "x_test, y_test, has_faces_test = create_dataset(filename2features_val, os.path.join(DATA_DIR, 'Val_AFEW'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "81aaa2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm,metrics,preprocessing\n",
    "\n",
    "x_train_norm=preprocessing.normalize(x_train,norm='l2')\n",
    "x_test_norm=preprocessing.normalize(x_test,norm='l2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "55c0e5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2010443864229765\n",
      "Complete accuracy: 0.2010443864229765\n"
     ]
    }
   ],
   "source": [
    "#clf = svm.LinearSVC(C=3) #0.5 1.1 0.6\n",
    "#clf = svm.SVC(kernel='rbf')\n",
    "np.random.seed(1)\n",
    "clf=RandomForestClassifier(n_estimators=1000,max_depth=7, n_jobs=-1)\n",
    "#clf=KNeighborsClassifier(n_neighbors=3,p=2)\n",
    "\n",
    "#import xgboost as xgb\n",
    "#clf = xgb.XGBClassifier(n_estimators=1000,use_label_encoder=False)\n",
    "\n",
    "if True:    \n",
    "    clf.fit(x_train_norm[has_faces_train==1], y_train[has_faces_train==1])\n",
    "    y_pred = clf.predict(x_test_norm)\n",
    "else:\n",
    "    clf.fit(x_train[has_faces_train==1], y_train[has_faces_train==1])\n",
    "    y_pred = clf.predict(x_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test[has_faces_test==1], y_pred[has_faces_test==1]))\n",
    "print(\"Complete accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cd19a6",
   "metadata": {},
   "source": [
    "## Validation with classifiers - 59%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "258b6560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from random import shuffle\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier,ExtraTreesClassifier\n",
    "from sklearn import svm,metrics,preprocessing\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import preprocessing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "#from scipy.misc import imread, imresize\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8d59c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR=r'D:\\Users\\amira\\Documents\\datasets\\emotions\\AudioVideo\\faces'\n",
    "IMG_SIZE = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9598949a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (conv_stem): Conv2dSame(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "  (bn1): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act1): SiLU(inplace=True)\n",
       "  (blocks): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): DepthwiseSeparableConv(\n",
       "        (conv_dw): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn1): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pw): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (conv_pw): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2dSame(96, 96, kernel_size=(3, 3), stride=(2, 2), groups=96, bias=False)\n",
       "        (bn2): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "        (bn2): BatchNorm2d(144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2dSame(144, 144, kernel_size=(5, 5), stride=(2, 2), groups=144, bias=False)\n",
       "        (bn2): BatchNorm2d(144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(40, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "        (bn2): BatchNorm2d(240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(40, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2dSame(240, 240, kernel_size=(3, 3), stride=(2, 2), groups=240, bias=False)\n",
       "        (bn2): BatchNorm2d(240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "        (bn2): BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "        (bn2): BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
       "        (bn2): BatchNorm2d(480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "        (bn2): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "        (bn2): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2dSame(672, 672, kernel_size=(5, 5), stride=(2, 2), groups=672, bias=False)\n",
       "        (bn2): BatchNorm2d(672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "        (bn2): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "        (bn2): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): InvertedResidual(\n",
       "        (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "        (bn2): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): InvertedResidual(\n",
       "        (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act1): SiLU(inplace=True)\n",
       "        (conv_dw): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "        (bn2): BatchNorm2d(1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act2): SiLU(inplace=True)\n",
       "        (se): SqueezeExcite(\n",
       "          (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act1): SiLU(inplace=True)\n",
       "          (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (conv_pwl): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv_head): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (bn2): BatchNorm2d(1280, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act2): SiLU(inplace=True)\n",
       "  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=True)\n",
       "  (classifier): Identity()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = r'D:\\Users\\amira\\openvino_notebooks\\notebooks\\accuracy_afew\\models\\enet_b0_8\\enet_b0_8.pt'\n",
    "\n",
    "feature_extractor_model = torch.load(PATH)\n",
    "feature_extractor_model.classifier=torch.nn.Identity()\n",
    "feature_extractor_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55d267aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((IMG_SIZE,IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47c930f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_to_index = {'Angry':0, 'Disgust':1, 'Fear':2, 'Happy':3, 'Neutral':4, 'Sad':5, 'Surprise':6}\n",
    "\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8ef10c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "954828b89a604a428bc4b8c24e47f1a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4afec6ce99dd4b3e94cf0250ef48adf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "166cf98923ad4a0b98de74891ad76e05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00e9d281ba4b422fa85bc43d38ec21df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d11ca26a205b431d9f1555f8180f2a52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60959306d21f4ace9d707ddf76a3efff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc75b97a8d1745d7b14453aa71e7d7c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f60fbc6c37154cf3b538deeb13475be6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/133 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "795ce09b7a0e46e18de5a2dd0c526248",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d3ae14abd714328b94cea6b1b59a6ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "661dbfc50a0a447d928c3bc80396c897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b97a6a60d40c4b5a887df27a4ddb9695",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/144 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d6190c3f11c4a34a2464233b33c1daa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/117 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "310721a9aca84c6888c10e1ca463c341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_features(data_dir):\n",
    "    filename2features={}\n",
    "    for class_name in os.listdir(data_dir):\n",
    "        if class_name in emotion_to_index:\n",
    "            class_dir = os.path.join(data_dir, class_name)\n",
    "        for video_name in tqdm(os.listdir(class_dir)):\n",
    "            frames_dir=os.path.join(class_dir,video_name)\n",
    "            X_global_features,X_isface=[],[]\n",
    "            imgs=[]\n",
    "            for img_name in os.listdir(frames_dir):\n",
    "                img = Image.open(os.path.join(frames_dir,img_name))\n",
    "                img_tensor = test_transforms(img)\n",
    "                X_isface.append('noface' not in img_name)\n",
    "                    \n",
    "                if img.size:\n",
    "                    imgs.append(img_tensor)\n",
    "                    if len(imgs)>=16:        \n",
    "                        #global_features,feats,scores=feature_extractor_model.predict(inp)\n",
    "                        scores = feature_extractor_model(torch.stack(imgs, dim=0).to(device))\n",
    "                        scores=scores.data.cpu().numpy()\n",
    "                        #print(scores.shape)\n",
    "                \n",
    "                        #print(global_features.shape,feats.shape,scores.shape)\n",
    "                        if len(X_global_features)==0:\n",
    "                            X_global_features=scores\n",
    "                        else:\n",
    "                            X_global_features=np.concatenate((X_global_features,scores),axis=0)\n",
    "                        \n",
    "                        imgs=[]\n",
    "\n",
    "            if len(imgs)>0:        \n",
    "                scores = feature_extractor_model(torch.stack(imgs, dim=0).to(device))\n",
    "                scores=scores.data.cpu().numpy()\n",
    "                #print(scores.shape)\n",
    "\n",
    "                #print(global_features.shape,feats.shape,scores.shape)\n",
    "                if len(X_global_features)==0:\n",
    "                    X_global_features=scores\n",
    "                else:\n",
    "                    X_global_features=np.concatenate((X_global_features,scores),axis=0)\n",
    "\n",
    "            X_isface=np.array(X_isface)\n",
    "            #print(X_global_features.shape,X_feats.shape,X_scores.shape)\n",
    "            filename2features[video_name]=(X_global_features,X_isface)\n",
    "    return filename2features\n",
    "\n",
    "filename2features_val=get_features(os.path.join(DATA_DIR, 'Val_AFEW'))\n",
    "filename2features_train=get_features(os.path.join(DATA_DIR, 'Train_AFEW'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5396a9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enet_b0_8_best_afew_feat_emotiw.pickle\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "model_name = 'enet_b0_8'\n",
    "MODEL2EMOTIW_FEATURES=model_name+'_best_afew_feat_emotiw.pickle' \n",
    "\n",
    "print(MODEL2EMOTIW_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95195c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(MODEL2EMOTIW_FEATURES, 'wb') as handle:\n",
    "    pickle.dump([filename2features_train,filename2features_val], handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "defe23b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "773 383\n"
     ]
    }
   ],
   "source": [
    "with open(MODEL2EMOTIW_FEATURES, 'rb') as handle:\n",
    "    filename2features_train,filename2features_val=pickle.load(handle)\n",
    "print(len(filename2features_train),len(filename2features_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4381568d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb8b904bcc524130b7bfe6f355ca666d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/133 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da954a7dbe034a56a326054f573ab812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9df5d4fdb75416abe18e75172561f3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce6ff302babc42909d2c435c3d684691",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17b5da609b5b46b3b724cee332a5adbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/144 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55e86bd035704d599d49e93edb5e4531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/117 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abca1f17cb2a4483a5447c1cb049ffed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/74 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(773, 5120) (773,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b60d6b5be4694f4b837c7077df1d2f1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9b2c3e3ca2d48c2855c39d88196ab9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1ef6e37ffce4c51b0d2431bacbcb26d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7c768bbffd840d9be91fd3460b7a1fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71bb3dc7321d4257b5fd72238306b406",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2c68a0ad61b40cdaf402b5c5f4605ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12c82b3408894357ba34c1efee222744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(383, 5120) (383,)\n"
     ]
    }
   ],
   "source": [
    "def create_dataset(filename2features,data_dir):\n",
    "    x = []\n",
    "    y = []\n",
    "    has_faces=[]\n",
    "    ind=0\n",
    "    for class_name in emotion_to_index:\n",
    "        for filename in tqdm(os.listdir(os.path.join(data_dir,class_name))):\n",
    "            fn=os.path.splitext(filename)[0] # goes through files names\n",
    "            if not fn in filename2features:\n",
    "                continue\n",
    "            features=filename2features[fn]\n",
    "            total_features=None\n",
    "            #print(len(features))\n",
    "            if True:\n",
    "                if len(features[0])!=0:\n",
    "                    cur_features=features[0][features[-1]==1]\n",
    "                #print(prev,features.shape)\n",
    "            else:\n",
    "                cur_features=features[0]\n",
    "            if len(cur_features)==0:\n",
    "                has_faces.append(0)\n",
    "                total_features=np.zeros_like(feature)\n",
    "            else:\n",
    "                has_faces.append(1)\n",
    "                #mean_features=features.mean(axis=0)\n",
    "                mean_features = (np.mean(cur_features, axis=0))\n",
    "                std_features = (np.std(cur_features, axis=0))\n",
    "                max_features = (np.max(cur_features, axis=0))\n",
    "                min_features = (np.min(cur_features, axis=0))\n",
    "\n",
    "                # join several features together\n",
    "                feature = np.concatenate((mean_features, std_features, min_features, max_features), axis=None)                    \n",
    "                #feature = np.concatenate((mean_features, std_features, min_features), axis=None)\n",
    "                #feature = np.concatenate((mean_features, min_features, max_features), axis=None)\n",
    "                #feature = np.concatenate((max_features, std_features), axis=None)\n",
    "                #feature=max_features\n",
    "\n",
    "                total_features=feature\n",
    "            \n",
    "            if total_features is not None:\n",
    "                x.append(total_features)\n",
    "                y.append(emotion_to_index[class_name])\n",
    "    x=np.array(x)\n",
    "    y=np.array(y)\n",
    "    has_faces=np.array(has_faces)\n",
    "    print(x.shape,y.shape)\n",
    "    return x,y,has_faces\n",
    "\n",
    "x_train, y_train, has_faces_train = create_dataset(filename2features_train, os.path.join(DATA_DIR, 'Train_AFEW'))\n",
    "x_test, y_test, has_faces_test = create_dataset(filename2features_val, os.path.join(DATA_DIR, 'Val_AFEW'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9dff020b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm,metrics,preprocessing\n",
    "\n",
    "x_train_norm=preprocessing.normalize(x_train,norm='l2')\n",
    "x_test_norm=preprocessing.normalize(x_test,norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c00df939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5963060686015831\n",
      "Complete accuracy: 0.5900783289817232\n"
     ]
    }
   ],
   "source": [
    "clf = svm.LinearSVC(C=1.1) #0.5 1.1 0.6\n",
    "#clf = svm.SVC(kernel='rbf')\n",
    "#np.random.seed(1)\n",
    "#clf=RandomForestClassifier(n_estimators=1000,max_depth=7, n_jobs=-1)\n",
    "#clf=KNeighborsClassifier(n_neighbors=3,p=2)\n",
    "\n",
    "#import xgboost as xgb\n",
    "#clf = xgb.XGBClassifier(n_estimators=1000,use_label_encoder=False)\n",
    "\n",
    "if True:    \n",
    "    clf.fit(x_train_norm[has_faces_train==1], y_train[has_faces_train==1])\n",
    "    y_pred = clf.predict(x_test_norm)\n",
    "else:\n",
    "    clf.fit(x_train[has_faces_train==1], y_train[has_faces_train==1])\n",
    "    y_pred = clf.predict(x_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test[has_faces_test==1], y_pred[has_faces_test==1]))\n",
    "print(\"Complete accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d5b0939b96e20240b6688bffb0ff6dfbaab1e456544936bbdb82861744a570ee"
  },
  "kernelspec": {
   "display_name": "openvino_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
